{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import string                            #导入字符串模块\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from unicodedata import normalize\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = open('word_n_standard.csv', 'r')\n",
    "reader = csv.reader(csvFile)\n",
    "standard_word = [] # 需要考虑的词集\n",
    "noun_n = 50\n",
    "source = [] # 存放产品分级的词频数组\n",
    "source_p = [] # 存放产品的词频数组\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in reader:\n",
    "    # 忽略第一行\n",
    "    if reader.line_num == 1:\n",
    "        continue\n",
    "    if reader.line_num > noun_n:\n",
    "        break\n",
    "    standard_word.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab1 = \"./hair_dryer.tsv\"\n",
    "tab2 = \".\"\n",
    "df_hd = pd.read_csv(tab1, sep='\\t', header=0)\n",
    "df_hd.head()\n",
    "df_hd = df_hd[~(df_hd['vine'].str.contains(\"N\") & df_hd['verified_purchase'].str.contains(\"N\"))]\n",
    "df_hd = df_hd[(df_hd['total_votes'] > 0) & (df_hd['helpful_votes'] > 0)]\n",
    "\n",
    "df_hd = df_hd.groupby('product_parent').filter(lambda x: len(x) > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/alphonse/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alphonse/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/alphonse/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/alphonse/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#import nltk resources\n",
    "resources = [\"wordnet\", \"stopwords\", \"punkt\", \\\n",
    "             \"averaged_perceptron_tagger\", \"maxent_treebank_pos_tagger\"]\n",
    "\n",
    "for resource in resources:\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/\" + resource)\n",
    "    except LookupError:\n",
    "        nltk.download(resource)\n",
    "\n",
    "#create Lemmatizer object\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_word(tagged_token):\n",
    "    \"\"\" Returns lemmatized word given its tag\"\"\"\n",
    "    root = []\n",
    "    for token in tagged_token:\n",
    "        tag = token[1][0]\n",
    "        word = token[0]\n",
    "        if tag.startswith('J'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADJ))\n",
    "        elif tag.startswith('V'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.VERB))\n",
    "        elif tag.startswith('N'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.NOUN))\n",
    "        elif tag.startswith('R'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADV))\n",
    "        else:          \n",
    "            root.append(word)\n",
    "    return root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_doc(document):\n",
    "    \"\"\" Tags words then returns sentence with lemmatized words\"\"\"\n",
    "    lemmatized_list = []\n",
    "    tokenized_sent = sent_tokenize(document)\n",
    "    for sentence in tokenized_sent:\n",
    "        no_punctuation = re.sub(r\"[`'\\\",.!?()]\", \" \", sentence)\n",
    "        tokenized_word = word_tokenize(no_punctuation)\n",
    "        tagged_token = pos_tag(tokenized_word)\n",
    "        lemmatized = lemmatize_word(tagged_token)\n",
    "        lemmatized_list.extend(lemmatized)\n",
    "    return \" \".join(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义处理函数\n",
    "def process_data(df_pp):\n",
    "    pattern = r\"\\&\\#[0-9]+\\;\"\n",
    "    df_pp[\"preprocessed\"] = df_pp[\"review_body\"].str.replace(pat=pattern, repl=\"\", regex=True)\n",
    "    \n",
    "    df_pp[\"preprocessed\"] = df_pp[\"preprocessed\"].apply(lambda row: lemmatize_doc(row))\n",
    "    \n",
    "    remove_accent = lambda text: normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "    df_pp[\"preprocessed\"] = df_pp[\"preprocessed\"].apply(remove_accent)\n",
    "    \n",
    "    pattern = r\"[^\\w\\s]\"\n",
    "    df_pp[\"preprocessed\"] = df_pp[\"preprocessed\"].str.replace(pat=pattern, repl=\" \", regex=True)\n",
    "\n",
    "    df_pp[\"preprocessed\"] = df_pp[\"preprocessed\"].str.lower()\n",
    "\n",
    "    pattern = r\"[\\s]+\"\n",
    "\n",
    "    df_pp[\"preprocessed\"] = df_pp[\"preprocessed\"].str.replace(pat=pattern, repl=\" \", regex=True)\n",
    "    \n",
    "    corpora = df_pp[\"preprocessed\"].values\n",
    "    tokenized = [corpus.split(\" \") for corpus in corpora]\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alphonse/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/alphonse/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/alphonse/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/alphonse/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/alphonse/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/alphonse/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# 遍历分组\n",
    "for key, group in df_hd.groupby('product_parent'):\n",
    "    product_r = list()\n",
    "    # 遍历星级\n",
    "    for star in range(1, 6):\n",
    "        star_r = [0] * noun_n\n",
    "        # df_pp 代表当前产品某一星级的数据\n",
    "        df_pp = group[group['star_rating'] == star]\n",
    "        tokenized = process_data(df_pp)\n",
    "        for review in tokenized:\n",
    "            for word in review:\n",
    "                if word in standard_word:\n",
    "                    star_r[standard_word.index(word)] = star_r[standard_word.index(word)] + 1\n",
    "        product_r.append(star_r)\n",
    "\n",
    "    source.append(product_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_p = []\n",
    "for product_r in source:\n",
    "    array = np.array(product_r[0])+ np.array(product_r[1]) + np.array(product_r[2]) + np.array(product_r[3]) + np.array(product_r[4])\n",
    "    source_p.append(array.tolist())\n",
    "source_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3 18 ...  1  2  0]\n",
      " [ 3  4 13 ...  2  1  0]\n",
      " [42 31 24 ...  4 13  0]\n",
      " ...\n",
      " [23  6 10 ...  0  3  0]\n",
      " [ 3  8 10 ...  1  1  0]\n",
      " [17  6  5 ...  0  2  0]]\n"
     ]
    }
   ],
   "source": [
    "print(source_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets.samples_generator import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.79535767e-01 1.66026988e-01 7.29231037e-02 5.78118273e-02\n",
      " 3.37271683e-02 2.52984550e-02 1.99203207e-02 1.89421612e-02\n",
      " 1.50351169e-02 1.26343029e-02 1.06053568e-02 9.25994051e-03\n",
      " 8.47945318e-03 7.69861351e-03 6.56949472e-03 5.85198529e-03\n",
      " 4.57129466e-03 4.46122065e-03 3.87382090e-03 3.48496811e-03\n",
      " 3.11020669e-03 3.04087587e-03 2.88249331e-03 2.50167847e-03\n",
      " 2.30819943e-03 2.12107894e-03 1.79542894e-03 1.60796767e-03\n",
      " 1.51075725e-03 1.41698255e-03 1.33692881e-03 1.18264069e-03\n",
      " 1.13629006e-03 8.99944876e-04 8.57343100e-04 7.86410795e-04\n",
      " 7.24119694e-04 6.34554723e-04 5.60876377e-04 5.44299597e-04\n",
      " 4.31917311e-04 3.86538237e-04 3.47258223e-04 3.16251982e-04\n",
      " 2.45814232e-04 2.04682402e-04 1.68182380e-04 1.42065644e-04\n",
      " 8.68524801e-05 3.31698358e-33]\n",
      "[5.15025128e+02 1.78314271e+02 7.83199782e+01 6.20903503e+01\n",
      " 3.62232400e+01 2.71707366e+01 2.13945787e+01 2.03440279e+01\n",
      " 1.61478320e+01 1.35693392e+01 1.13902354e+01 9.94524783e+00\n",
      " 9.10699839e+00 8.26837053e+00 7.05568821e+00 6.28507753e+00\n",
      " 4.90960588e+00 4.79138553e+00 4.16051365e+00 3.74288274e+00\n",
      " 3.34038607e+00 3.26592423e+00 3.09582014e+00 2.68682205e+00\n",
      " 2.47902407e+00 2.27805521e+00 1.92830459e+00 1.72696973e+00\n",
      " 1.62256499e+00 1.52185023e+00 1.43587190e+00 1.27016526e+00\n",
      " 1.22038432e+00 9.66547767e-01 9.20793130e-01 8.44611284e-01\n",
      " 7.77710159e-01 6.81516687e-01 6.02385573e-01 5.84581983e-01\n",
      " 4.63882537e-01 4.15145061e-01 3.72958021e-01 3.39657078e-01\n",
      " 2.64006389e-01 2.19830485e-01 1.80629179e-01 1.52579602e-01\n",
      " 9.32802364e-02 3.56246606e-30]\n"
     ]
    }
   ],
   "source": [
    "#先不降维，只对数据进行投影，看看投影后的三个维度的方差分布\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50)\n",
    "pca.fit(X)\n",
    "#返回所保留的n个成分各自的方差百分比\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47953577 0.16602699 0.0729231  0.05781183 0.03372717 0.02529846\n",
      " 0.01992032 0.01894216 0.01503512 0.0126343 ]\n",
      "[515.02512816 178.31427072  78.31997819  62.09035026  36.22323995\n",
      "  27.17073661  21.39457869  20.34402786  16.14783201  13.56933919]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#进行降维，从三维降到20维\n",
    "pca1 = PCA(n_components=0.9)\n",
    "pca1.fit(X)\n",
    "#返回所保留的n个成分各自的方差百分比\n",
    "print(pca1.explained_variance_ratio_)\n",
    "print(pca1.explained_variance_)\n",
    "\n",
    "print(pca1.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
