{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import string                            #导入字符串模块\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_parent</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>758099411</th>\n",
       "      <td>2186</td>\n",
       "      <td>1662</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47684938</th>\n",
       "      <td>2135</td>\n",
       "      <td>877</td>\n",
       "      <td>951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732252283</th>\n",
       "      <td>2270</td>\n",
       "      <td>625</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127343313</th>\n",
       "      <td>929</td>\n",
       "      <td>534</td>\n",
       "      <td>594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694290590</th>\n",
       "      <td>1489</td>\n",
       "      <td>483</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357308868</th>\n",
       "      <td>811</td>\n",
       "      <td>438</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194533684</th>\n",
       "      <td>144</td>\n",
       "      <td>429</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531479992</th>\n",
       "      <td>629</td>\n",
       "      <td>419</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614083399</th>\n",
       "      <td>571</td>\n",
       "      <td>403</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195677102</th>\n",
       "      <td>591</td>\n",
       "      <td>340</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                star_rating  helpful_votes  total_votes\n",
       "product_parent                                         \n",
       "758099411              2186           1662         1987\n",
       "47684938               2135            877          951\n",
       "732252283              2270            625          714\n",
       "127343313               929            534          594\n",
       "694290590              1489            483          524\n",
       "357308868               811            438          515\n",
       "194533684               144            429          453\n",
       "531479992               629            419          476\n",
       "614083399               571            403          489\n",
       "195677102               591            340          422"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab1 = \"./hair_dryer.tsv\"\n",
    "\n",
    "df = pd.read_csv(tab1, sep='\\t', header=0)\n",
    "\n",
    "df = df[~(df['vine'].str.contains(\"N\") & df['verified_purchase'].str.contains(\"N\"))]\n",
    "\n",
    "df = df.groupby('product_parent').filter(lambda x: len(x) > 1)\n",
    "\n",
    "gp_pp = df.groupby('product_parent')\n",
    "\n",
    "# # 选特定的一组\n",
    "# print(gp_pp.get_group(732252283)['star_rating'])\n",
    "\n",
    "# # 遍历\n",
    "# for item in gp_pp:\n",
    "#     print(item[0])\n",
    "#     print(item[1]['star_rating'])\n",
    "\n",
    "# gp_pp[['star_rating', 'helpful_votes', 'total_votes']].sum()\n",
    "\n",
    "gp_pp[['star_rating', 'helpful_votes', 'total_votes']].sum().sort_values(by='helpful_votes',ascending=False).head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/alphonse/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alphonse/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/alphonse/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/alphonse/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# 定义功能函数\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "#import nltk resources\n",
    "resources = [\"wordnet\", \"stopwords\", \"punkt\", \\\n",
    "             \"averaged_perceptron_tagger\", \"maxent_treebank_pos_tagger\"]\n",
    "\n",
    "for resource in resources:\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/\" + resource)\n",
    "    except LookupError:\n",
    "        nltk.download(resource)\n",
    "\n",
    "#create Lemmatizer object\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_word(tagged_token):\n",
    "    \"\"\" Returns lemmatized word given its tag\"\"\"\n",
    "    root = []\n",
    "    for token in tagged_token:\n",
    "        tag = token[1][0]\n",
    "        word = token[0]\n",
    "        if tag.startswith('J'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADJ))\n",
    "        elif tag.startswith('V'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.VERB))\n",
    "        elif tag.startswith('N'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.NOUN))\n",
    "        elif tag.startswith('R'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADV))\n",
    "        else:          \n",
    "            root.append(word)\n",
    "    return root\n",
    "\n",
    "def lemmatize_doc(document):\n",
    "    \"\"\" Tags words then returns sentence with lemmatized words\"\"\"\n",
    "    lemmatized_list = []\n",
    "    tokenized_sent = sent_tokenize(document)\n",
    "    for sentence in tokenized_sent:\n",
    "        no_punctuation = re.sub(r\"[`'\\\",.!?()]\", \" \", sentence)\n",
    "        tokenized_word = word_tokenize(no_punctuation)\n",
    "        tagged_token = pos_tag(tokenized_word)\n",
    "        lemmatized = lemmatize_word(tagged_token)\n",
    "        lemmatized_list.extend(lemmatized)\n",
    "    return \" \".join(lemmatized_list)\n",
    "\n",
    "\n",
    "from unicodedata import normalize\n",
    "\n",
    "remove_accent = lambda text: normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "adjective_tags = ['JJ','JJR','JJS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_parents = [758099411, 47684938, 732252283, 127343313, 694290590, 357308868, 194533684, 531479992, 614083399]\n",
    "\n",
    "product_parents = [758099411, 47684938, 732252283]\n",
    "num_PSW = []\n",
    "score_S = [-0.5, -1, 0, 0.5, 1]\n",
    "num_PSR = [] * len(product_parents) \n",
    "score_PSW = []\n",
    "\n",
    "keywords = [] # 需要考虑的词集\n",
    "scores = [] # 关键词得分\n",
    "keywords_num = 20\n",
    "\n",
    "del_word = ['ta', 'diff', 'low', 'ur', 'oo']\n",
    "change_word = {'yr': 'year', 'hr': 'hour', 'mo': 'moment', 'ra': 'rate adaptation'}\n",
    "\n",
    "csvFile = open('keyword_20_noun.csv', 'r')\n",
    "reader = csv.reader(csvFile)\n",
    "for item in reader:\n",
    "    # 忽略第一行\n",
    "    if reader.line_num == 1:\n",
    "        continue\n",
    "    if reader.line_num > keywords_num + 1:\n",
    "        break\n",
    "    keywords.append(item[0])\n",
    "    scores.append(float(item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alphonse/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/alphonse/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/home/alphonse/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/alphonse/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/alphonse/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/alphonse/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "id_i = 0\n",
    "for id in product_parents:\n",
    "    num_PSR.append(list())\n",
    "    num_PSW.append(list())\n",
    "    df_pp = gp_pp.get_group(id)\n",
    "\n",
    "    for star in range(1, 6):\n",
    "        star_scores = [0] * keywords_num\n",
    "        df = df_pp[df_pp['star_rating'] == star]\n",
    "        num_PSR[id_i].append(len(df))\n",
    "        \n",
    "\n",
    "        pattern = r\"\\&\\#[0-9]+\\;\"\n",
    "        df[\"preprocessed\"] = df[\"review_body\"].str.replace(pat=pattern, repl=\"\", regex=True)\n",
    "        df[\"preprocessed\"] = df[\"preprocessed\"].apply(lambda row: lemmatize_doc(row))\n",
    "        df[\"preprocessed\"] = df[\"preprocessed\"].apply(remove_accent)\n",
    "        pattern = r\"[^\\w\\s]\"\n",
    "        df[\"preprocessed\"] = df[\"preprocessed\"].str.replace(pat=pattern, repl=\" \", regex=True)\n",
    "        df[\"preprocessed\"] = df[\"preprocessed\"].str.lower()\n",
    "        pattern = r\"[\\s]+\"\n",
    "        df[\"preprocessed\"] = df[\"preprocessed\"].str.replace(pat=pattern, repl=\" \", regex=True)\n",
    "        corpora = df[\"preprocessed\"].values\n",
    "        tokenized = [corpus.split(\" \") for corpus in corpora]\n",
    "        for review in tokenized:\n",
    "            while '' in review:\n",
    "                review.remove('')\n",
    "        for text in tokenized:\n",
    "            selected = [0] * keywords_num\n",
    "            POS_tag = nltk.pos_tag(text)\n",
    "            lemmatized_text = []\n",
    "            for word in POS_tag:\n",
    "                if word[1] in adjective_tags:\n",
    "                    lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
    "                else:\n",
    "                    lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n",
    "            POS_tag = nltk.pos_tag(lemmatized_text)\n",
    "            stopwords = []\n",
    "            wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n",
    "\n",
    "            for word in POS_tag:\n",
    "                if word[1] not in wanted_POS:\n",
    "                    stopwords.append(word[0])\n",
    "\n",
    "            punctuations = list(str(string.punctuation))\n",
    "\n",
    "            stopwords = stopwords + punctuations\n",
    "            stopword_file = open(\"long_stopwords.txt\", \"r\")\n",
    "\n",
    "            lots_of_stopwords = []\n",
    "\n",
    "            for line in stopword_file.readlines():\n",
    "                lots_of_stopwords.append(str(line.strip()))\n",
    "\n",
    "            stopwords_plus = []\n",
    "            stopwords_plus = stopwords + lots_of_stopwords\n",
    "            stopwords_plus = set(stopwords_plus)\n",
    "\n",
    "            processed_text = []\n",
    "            for word in lemmatized_text:\n",
    "                if word not in stopwords_plus:\n",
    "                    processed_text.append(word)\n",
    "\n",
    "            vocabulary = list(set(processed_text))\n",
    "            phrases = []\n",
    "            phrase = \" \"\n",
    "            for word in lemmatized_text:\n",
    "                \n",
    "                if word in stopwords_plus:\n",
    "                    if phrase!= \" \":\n",
    "                        phrases.append(str(phrase).strip().split())\n",
    "                    phrase = \" \"\n",
    "                elif word not in stopwords_plus:\n",
    "                    phrase+=str(word)\n",
    "                    phrase+=\" \"\n",
    "\n",
    "            for word in vocabulary:\n",
    "                for phrase in phrases:\n",
    "                    if (word in phrase) and (word in phrases) and (len(phrase)>1):\n",
    "                        #if len(phrase)>1 then the current phrase is multi-worded.\n",
    "                        #if the word in vocabulary is present in unique_phrases as a single-word-phrase\n",
    "                        # and at the same time present as a word within a multi-worded phrase,\n",
    "                        # then I will remove the single-word-phrase from the list.\n",
    "                        phrases.remove([word])\n",
    "            \n",
    "            for phrase in phrases:\n",
    "                word = ' '\n",
    "                for word in phrase:\n",
    "                    if (word in keywords) and (selected[keywords.index(word)] != 1):\n",
    "                        star_scores[keywords.index(word)] = star_scores[keywords.index(word)] + 1 - selected[keywords.index(word)]\n",
    "                        selected[keywords.index(word)] = 1\n",
    "                    else:\n",
    "                        for i in range(0, len(keywords)):\n",
    "                            if word in change_word.keys():\n",
    "                                word = change_word[word]\n",
    "                            if (word in keywords[i]) and (word not in del_word) and (selected[i] == 0):\n",
    "                                star_scores[i] = star_scores[i] + 0.5\n",
    "                                selected[i] = 0.5\n",
    "\n",
    "        num_PSW[id_i].append(star_scores)\n",
    "    id_i = id_i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, h = 1, v = 1):\n",
    "    return v * 1 / (1 + math.exp(- x / h * 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.998849042892154,\n",
       " 7.068161945583372,\n",
       " 8.877467553235721,\n",
       " 8.994166675956867,\n",
       " 8.983355555932674,\n",
       " 7.21146313297712,\n",
       " 8.86716736322503,\n",
       " 8.856373379774112,\n",
       " 8.82026716669167,\n",
       " 8.856373379774112,\n",
       " 7.4696898025650045,\n",
       " 8.988078384190498,\n",
       " 8.826591759409766,\n",
       " 4.621431795187287,\n",
       " 8.840257852193714,\n",
       " 8.705512885941987,\n",
       " 5.839140312774788,\n",
       " 8.615013932543823,\n",
       " 8.628207932603239,\n",
       " 4.573890367332127]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_PSW_sigmoid = []\n",
    "for i in range(len(product_parents)):\n",
    "    score_PSW.append(list())\n",
    "    score_PSW_sigmoid.append(list())\n",
    "    for j in range(keywords_num):\n",
    "        res = 0\n",
    "        for k in range(5):\n",
    "            res = res + score_S[k] * num_PSW[i][k][j] / math.sqrt(num_PSR[i][k])\n",
    "        score_PSW[i].append(res)\n",
    "        score_PSW_sigmoid[i].append(sigmoid(res, 1.9, 9))\n",
    "\n",
    "display(score_PSW_sigmoid[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3714244034930954"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(list(chain(*score_PSW)), 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 层次分析法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算比较矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(keywords_num):\n",
    "    B.append(list())\n",
    "    for j in range(len(product_parents)):\n",
    "        B[i].append(list())\n",
    "        for k in range(len(product_parents)):\n",
    "            B[i][j].append(score_PSW_sigmoid[j][i] /  score_PSW_sigmoid[k][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算准则层到目标层矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch(x):\n",
    "    return (x - 1) * 4 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = []\n",
    "for i in range(0, keywords_num):\n",
    "    A.append(list())\n",
    "    for j in range(0, keywords_num):\n",
    "        res = scores[i] / scores[j]\n",
    "        if res > 1:\n",
    "            A[i].append(stretch(res))\n",
    "        elif res < 1:\n",
    "            A[i].append(1/stretch(scores[j] / scores[i]))\n",
    "        else:\n",
    "            A[i].append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始进行层次分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-154-d9b9cbb7f45b>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-154-d9b9cbb7f45b>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    CI = (max_eigen - n) / (n - 1)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "class AHP:\n",
    "    def __init__(self, criteria, b):\n",
    "        self.RI = (0, 0, 0.58, 0.9, 1.12, 1.24, 1.32, 1.41, 1.45, 1.49)\n",
    "        self.criteria = criteria\n",
    "        self.b = b\n",
    "        self.num_criteria = criteria.shape[0]\n",
    "        self.num_project = b[0].shape[0]\n",
    "\n",
    "    def cal_weights(self, input_matrix):\n",
    "        input_matrix = np.array(input_matrix)\n",
    "        n, n1 = input_matrix.shape\n",
    "        assert n == n1, '不是一个方阵'\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if np.abs(input_matrix[i, j] * input_matrix[j, i] - 1) > 1e-7:\n",
    "                    raise ValueError('不是反互对称矩阵')\n",
    "\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(input_matrix)\n",
    "\n",
    "        max_idx = np.argmax(eigenvalues)\n",
    "        max_eigen = eigenvalues[max_idx].real\n",
    "        eigen = eigenvectors[:, max_idx].real\n",
    "        eigen = eigen / eigen.sum()\n",
    "\n",
    "        if n > 9:\n",
    "            CR = None\n",
    "            warnings.warn('无法判断一致性')\n",
    "        else:\n",
    "            CI = (max_eigen - n) / (n - 1)\n",
    "            CR = CI / self.RI[n]\n",
    "        return max_eigen, CR, eigen\n",
    "\n",
    "    def run(self):\n",
    "        max_eigen, CR, criteria_eigen = self.cal_weights(self.criteria)\n",
    "        print(type(CR))\n",
    "        print('准则层：最大特征值{:<5f},CR={:<5f},检验{}通过'.format(max_eigen, CR, '' if CR < 0.1 else '不'))\n",
    "        print('准则层权重={}\\n'.format(criteria_eigen))\n",
    "\n",
    "        max_eigen_list, CR_list, eigen_list = [], [], []\n",
    "        for i in self.b:\n",
    "            max_eigen, CR, eigen = self.cal_weights(i)\n",
    "            max_eigen_list.append(max_eigen)\n",
    "            CR_list.append(CR)\n",
    "            eigen_list.append(eigen)\n",
    "\n",
    "        pd_print = pd.DataFrame(eigen_list,\n",
    "                                index=['准则' + str(i) for i in range(self.num_criteria)],\n",
    "                                columns=['方案' + str(i) for i in range(self.num_project)],\n",
    "                                )\n",
    "        pd_print.loc[:, '最大特征值'] = max_eigen_list\n",
    "        pd_print.loc[:, 'CR'] = CR_list\n",
    "        pd_print.loc[:, '一致性检验'] = pd_print.loc[:, 'CR'] < 0.1\n",
    "        print('方案层')\n",
    "        print(pd_print)\n",
    "\n",
    "        # 目标层\n",
    "        obj = np.dot(criteria_eigen.reshape(1, -1), np.array(eigen_list))\n",
    "        print('\\n目标层', obj)\n",
    "        print('最优选择是方案{}'.format(np.argmax(obj)))\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准则重要性矩阵\n",
    "criteria = np.array(A)\n",
    "\n",
    "plan = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in B:\n",
    "    plan.append(np.array(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AHP(criteria, plan).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
