{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import string                            #导入字符串模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab1 = \"./hair_dryer.tsv\"\n",
    "\n",
    "df = pd.read_csv(tab1, sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 删除无效信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~(df['vine'].str.contains(\"N\") & df['verified_purchase'].str.contains(\"N\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['total_votes'] > 0) & (df['helpful_votes'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('product_parent').filter(lambda x: len(x) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2775\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 整体文本分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"\\&\\#[0-9]+\\;\"\n",
    "\n",
    "df[\"preprocessed\"] = df[\"review_body\"].str.replace(pat=pattern, repl=\"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ''\n",
    "for review in df['review_body']:\n",
    "    reviews = reviews + ' ' + review\n",
    "hist = {}                                 #创建一个空字典，放词频与单词，无序排列\n",
    "data = []                                 #创建一个空列表，放词频与单词，有序：从多到少\n",
    "content = reviews.replace('-',' ')       #连字符—用空格代替\n",
    "words = content.split()                   #字符串按空格分割--分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excludes={\"not\",\"if\",\"about\",\"really\",\"too\",\"br\",\"has\",\"very\", \"so\",\"on\",\"at\",\"when\", \"was\",\"one\",\"had\", \"it's\",\"than\",\"would\",\"the\",\"and\",\"of\",\"you\",\"a\",\"with\",\"but\",\"as\",\"be\",\"in\",\"or\",\"are\", \"i\", \"it\", \"to\", \"hair\",\"this\", \"is\", \"my\", \"dryer\", \"for\", \"that\", \"have\"}\n",
    "# for word in excludes:\n",
    "#     if word in hist:\n",
    "#         del(hist[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/alphonse/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alphonse/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/alphonse/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/alphonse/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#import nltk resources\n",
    "resources = [\"wordnet\", \"stopwords\", \"punkt\", \\\n",
    "             \"averaged_perceptron_tagger\", \"maxent_treebank_pos_tagger\"]\n",
    "\n",
    "for resource in resources:\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/\" + resource)\n",
    "    except LookupError:\n",
    "        nltk.download(resource)\n",
    "\n",
    "#create Lemmatizer object\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_word(tagged_token):\n",
    "    \"\"\" Returns lemmatized word given its tag\"\"\"\n",
    "    root = []\n",
    "    for token in tagged_token:\n",
    "        tag = token[1][0]\n",
    "        word = token[0]\n",
    "        if tag.startswith('J'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADJ))\n",
    "        elif tag.startswith('V'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.VERB))\n",
    "        elif tag.startswith('N'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.NOUN))\n",
    "        elif tag.startswith('R'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADV))\n",
    "        else:          \n",
    "            root.append(word)\n",
    "    return root\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_doc(document):\n",
    "    \"\"\" Tags words then returns sentence with lemmatized words\"\"\"\n",
    "    lemmatized_list = []\n",
    "    tokenized_sent = sent_tokenize(document)\n",
    "    for sentence in tokenized_sent:\n",
    "        no_punctuation = re.sub(r\"[`'\\\",.!?()]\", \" \", sentence)\n",
    "        tokenized_word = word_tokenize(no_punctuation)\n",
    "        tagged_token = pos_tag(tokenized_word)\n",
    "        lemmatized = lemmatize_word(tagged_token)\n",
    "        lemmatized_list.extend(lemmatized)\n",
    "    return \" \".join(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found this gem in a hotel in Savannah and like it so much I take a photo and find it on Amazon Bought two one for each bathroom\n"
     ]
    }
   ],
   "source": [
    "#apply our functions\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].apply(lambda row: lemmatize_doc(row))\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quickly dry your hair without fry it~\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "remove_accent = lambda text: normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].apply(remove_accent)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[465])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quickly dry your hair without fry it \n"
     ]
    }
   ],
   "source": [
    "pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].str.replace(pat=pattern, repl=\" \", regex=True)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[465])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quickly dry your hair without fry it \n"
     ]
    }
   ],
   "source": [
    "df[\"preprocessed\"] = df[\"preprocessed\"].str.lower()\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[465])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample stop words: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', 'youll', 'youd', 'your', 'yours'] \n",
      "\n",
      "quickly dry hair without fry \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "stop_words = [word.replace(\"\\'\", \"\") for word in stop_words]\n",
    "\n",
    "print(f\"sample stop words: {stop_words[:15]} \\n\")\n",
    "\n",
    "remove_stop_words = lambda row: \" \".join([token for token in row.split(\" \") \\\n",
    "                                          if token not in stop_words])\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].apply(remove_stop_words)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[465])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quickly dry hair without fry \n"
     ]
    }
   ],
   "source": [
    "pattern = r\"[\\s]+\"\n",
    "\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].str.replace(pat=pattern, repl=\" \", regex=True)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[465])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2nd', 'time', 'buy', 'little', 'hair', 'dryer', 'like', 'much', 'first', 'one', 'finally', 'bite', 'dust', '3', 'year', 'small', 'lightweight', 'plus', 'suitcase', 'still', 'powerful', 'enough', 'blow', 'dry', 'hair', 'fast', 'short', 'hair', 'sweet', 'little', 'dryer', 'need', 'dry', 'style', 'hair', 'need', 'comb', 'brush', 'frill', '2', 'speed', 'high', 'low', 'exactly', 'need', 'fast']\n"
     ]
    }
   ],
   "source": [
    "corpora = df[\"preprocessed\"].values\n",
    "tokenized = [corpus.split(\" \") for corpus in corpora]\n",
    "\n",
    "for review in tokenized:\n",
    "    while '' in review:\n",
    "        review.remove('')\n",
    "\n",
    "text = tokenized[4]\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging For Lemmatization\n",
    "\n",
    "NLTK is again used for <b>POS tagging</b> the input text so that the words can be lemmatized based on their POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text with POS tags: \n",
      "\n",
      "[('2nd', 'CD'), ('time', 'NN'), ('buy', 'VB'), ('little', 'JJ'), ('hair', 'JJ'), ('dryer', 'NN'), ('like', 'IN'), ('much', 'JJ'), ('first', 'JJ'), ('one', 'CD'), ('finally', 'RB'), ('bite', 'VB'), ('dust', 'NN'), ('3', 'CD'), ('year', 'NN'), ('small', 'JJ'), ('lightweight', 'JJ'), ('plus', 'CC'), ('suitcase', 'NN'), ('still', 'RB'), ('powerful', 'JJ'), ('enough', 'RB'), ('blow', 'NN'), ('dry', 'JJ'), ('hair', 'NN'), ('fast', 'RB'), ('short', 'JJ'), ('hair', 'NN'), ('sweet', 'JJ'), ('little', 'JJ'), ('dryer', 'NN'), ('need', 'VBP'), ('dry', 'JJ'), ('style', 'NN'), ('hair', 'NN'), ('need', 'VBP'), ('comb', 'NN'), ('brush', 'NN'), ('frill', 'NN'), ('2', 'CD'), ('speed', 'NN'), ('high', 'JJ'), ('low', 'JJ'), ('exactly', 'RB'), ('need', 'VBP'), ('fast', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "POS_tag = nltk.pos_tag(text)\n",
    "\n",
    "print (\"Tokenized Text with POS tags: \\n\")\n",
    "print (POS_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "The tokenized text (mainly the nouns and adjectives) is normalized by <b>lemmatization</b>.\n",
    "In lemmatization different grammatical counterparts of a word will be replaced by single\n",
    "basic lemma. For example, 'glasses' may be replaced by 'glass'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text tokens after lemmatization of adjectives and nouns: \n",
      "\n",
      "['2nd', 'time', 'buy', 'little', 'hair', 'dryer', 'like', 'much', 'first', 'one', 'finally', 'bite', 'dust', '3', 'year', 'small', 'lightweight', 'plus', 'suitcase', 'still', 'powerful', 'enough', 'blow', 'dry', 'hair', 'fast', 'short', 'hair', 'sweet', 'little', 'dryer', 'need', 'dry', 'style', 'hair', 'need', 'comb', 'brush', 'frill', '2', 'speed', 'high', 'low', 'exactly', 'need', 'fast']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "adjective_tags = ['JJ','JJR','JJS']\n",
    "\n",
    "lemmatized_text = []\n",
    "\n",
    "for word in POS_tag:\n",
    "    if word[1] in adjective_tags:\n",
    "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
    "    else:\n",
    "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n",
    "        \n",
    "print (\"Text tokens after lemmatization of adjectives and nouns: \\n\")\n",
    "print (lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging for Filtering\n",
    "\n",
    "The <b>lemmatized text</b> is <b>POS tagged</b> here. The tags will be used for filtering later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized text with POS tags: \n",
      "\n",
      "[('2nd', 'CD'), ('time', 'NN'), ('buy', 'VB'), ('little', 'JJ'), ('hair', 'JJ'), ('dryer', 'NN'), ('like', 'IN'), ('much', 'JJ'), ('first', 'JJ'), ('one', 'CD'), ('finally', 'RB'), ('bite', 'VB'), ('dust', 'NN'), ('3', 'CD'), ('year', 'NN'), ('small', 'JJ'), ('lightweight', 'JJ'), ('plus', 'CC'), ('suitcase', 'NN'), ('still', 'RB'), ('powerful', 'JJ'), ('enough', 'RB'), ('blow', 'NN'), ('dry', 'JJ'), ('hair', 'NN'), ('fast', 'RB'), ('short', 'JJ'), ('hair', 'NN'), ('sweet', 'JJ'), ('little', 'JJ'), ('dryer', 'NN'), ('need', 'VBP'), ('dry', 'JJ'), ('style', 'NN'), ('hair', 'NN'), ('need', 'VBP'), ('comb', 'NN'), ('brush', 'NN'), ('frill', 'NN'), ('2', 'CD'), ('speed', 'NN'), ('high', 'JJ'), ('low', 'JJ'), ('exactly', 'RB'), ('need', 'VBP'), ('fast', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "POS_tag = nltk.pos_tag(lemmatized_text)\n",
    "\n",
    "print (\"Lemmatized text with POS tags: \\n\")\n",
    "print (POS_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Based Filtering\n",
    "\n",
    "Any word from the lemmatized text, which isn't a noun, adjective, or gerund (or a 'foreign word'), is here\n",
    "considered as a <b>stopword</b> (non-content). This is based on the assumption that usually keywords are noun,\n",
    "adjectives or gerunds. \n",
    "\n",
    "Punctuations are added to the stopword list too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "\n",
    "wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n",
    "\n",
    "for word in POS_tag:\n",
    "    if word[1] not in wanted_POS:\n",
    "        stopwords.append(word[0])\n",
    "\n",
    "punctuations = list(str(string.punctuation))\n",
    "\n",
    "stopwords = stopwords + punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete stopword generation\n",
    "\n",
    "Even if we remove the aforementioned stopwords, still some extremely common nouns, adjectives or gerunds may\n",
    "remain which are very bad candidates for being keywords (or part of it). \n",
    "\n",
    "An external file constituting a long list of stopwords is loaded and all the words are added with the previous\n",
    "stopwords to create the final list 'stopwords-plus' which is then converted into a set. \n",
    "\n",
    "(Source of stopwords data: https://www.ranks.nl/stopwords)\n",
    "\n",
    "Stopwords-plus constitute the sum total of all stopwords and potential phrase-delimiters. \n",
    "\n",
    "(The contents of this set will be later used to partition the lemmatized text into n-gram phrases. But, for now, I will simply remove the stopwords, and work with a 'bag-of-words' approach. I will be developing the graph using unigram texts as vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_file = open(\"long_stopwords.txt\", \"r\")\n",
    "#Source = https://www.ranks.nl/stopwords\n",
    "\n",
    "lots_of_stopwords = []\n",
    "\n",
    "for line in stopword_file.readlines():\n",
    "    lots_of_stopwords.append(str(line.strip()))\n",
    "\n",
    "stopwords_plus = []\n",
    "stopwords_plus = stopwords + lots_of_stopwords\n",
    "stopwords_plus = set(stopwords_plus)\n",
    "\n",
    "#Stopwords_plus contain total set of all stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords \n",
    "\n",
    "Removing stopwords from lemmatized_text. \n",
    "Processeced_text condtains the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time', 'hair', 'dryer', 'dust', 'year', 'small', 'lightweight', 'suitcase', 'powerful', 'blow', 'dry', 'hair', 'short', 'hair', 'sweet', 'dryer', 'dry', 'style', 'hair', 'comb', 'brush', 'frill', 'speed', 'high', 'low']\n"
     ]
    }
   ],
   "source": [
    "processed_text = []\n",
    "for word in lemmatized_text:\n",
    "    if word not in stopwords_plus:\n",
    "        processed_text.append(word)\n",
    "print (processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Creation\n",
    "Vocabulary will only contain unique words from processed_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dry', 'year', 'suitcase', 'hair', 'high', 'short', 'brush', 'comb', 'low', 'small', 'style', 'frill', 'powerful', 'dryer', 'blow', 'sweet', 'time', 'dust', 'lightweight', 'speed']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(set(processed_text))\n",
    "print (vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Graph\n",
    "\n",
    "TextRank is a graph based model, and thus it requires us to build a graph. Each words in the vocabulary will serve as a vertex for graph. The words will be represented in the vertices by their index in vocabulary list.  \n",
    "\n",
    "The weighted_edge matrix contains the information of edge connections among all vertices.\n",
    "I am building wieghted undirected edges.\n",
    "\n",
    "weighted_edge[i][j] contains the weight of the connecting edge between the word vertex represented by vocabulary index i and the word vertex represented by vocabulary j.\n",
    "\n",
    "If weighted_edge[i][j] is zero, it means no edge connection is present between the words represented by index i and j.\n",
    "\n",
    "There is a connection between the words (and thus between i and j which represents them) if the words co-occur within a window of a specified 'window_size' in the processed_text.\n",
    "\n",
    "The value of the weighted_edge[i][j] is increased by (1/(distance between positions of words currently represented by i and j)) for every connection discovered between the same words in different locations of the text. \n",
    "\n",
    "The covered_coocurrences list (which is contain the list of pairs of absolute positions in processed_text of the words whose coocurrence at that location is already checked) is managed so that the same two words located in the same positions in processed_text are not repetitively counted while sliding the window one text unit at a time.\n",
    "\n",
    "The score of all vertices are intialized to one. \n",
    "\n",
    "Self-connections are not considered, so weighted_edge[i][i] will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "vocab_len = len(vocabulary)\n",
    "\n",
    "weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
    "\n",
    "score = np.zeros((vocab_len),dtype=np.float32)\n",
    "window_size = 3\n",
    "covered_coocurrences = []\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    score[i]=1\n",
    "    for j in range(0,vocab_len):\n",
    "        if j==i:\n",
    "            weighted_edge[i][j]=0\n",
    "        else:\n",
    "            for window_start in range(0,(len(processed_text)-window_size)):\n",
    "                \n",
    "                window_end = window_start+window_size\n",
    "                \n",
    "                window = processed_text[window_start:window_end]\n",
    "                \n",
    "                if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "                    \n",
    "                    index_of_i = window_start + window.index(vocabulary[i])\n",
    "                    index_of_j = window_start + window.index(vocabulary[j])\n",
    "                    \n",
    "                    # index_of_x is the absolute position of the xth term in the window \n",
    "                    # (counting from 0) \n",
    "                    # in the processed_text\n",
    "                      \n",
    "                    if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
    "                        covered_coocurrences.append([index_of_i,index_of_j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating weighted summation of connections of a vertex\n",
    "\n",
    "inout[i] will contain the sum of all the undirected connections\\edges associated withe the vertex represented by i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "inout = np.zeros((vocab_len),dtype=np.float32)\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    for j in range(0,vocab_len):\n",
    "        inout[i]+=weighted_edge[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Vertices\n",
    "\n",
    "The formula used for scoring a vertex represented by i is:\n",
    "\n",
    "score[i] = (1-d) + d x [ Summation(j) ( (weighted_edge[i][j]/inout[j]) x score[j] ) ] where j belongs to the list of vertieces that has a connection with i. \n",
    "\n",
    "d is the damping factor.\n",
    "\n",
    "The score is iteratively updated until convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converging at iteration 28....\n"
     ]
    }
   ],
   "source": [
    "MAX_ITERATIONS = 50\n",
    "d=0.85\n",
    "threshold = 0.0001 #convergence threshold\n",
    "\n",
    "for iter in range(0,MAX_ITERATIONS):\n",
    "    prev_score = np.copy(score)\n",
    "    \n",
    "    for i in range(0,vocab_len):\n",
    "        \n",
    "        summation = 0\n",
    "        for j in range(0,vocab_len):\n",
    "            if weighted_edge[i][j] != 0:\n",
    "                summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
    "                \n",
    "        score[i] = (1-d) + d*(summation)\n",
    "    \n",
    "    if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n",
    "        print(\"Converging at iteration \"+str(iter)+\"....\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of dry: 1.4889517\n",
      "Score of year: 0.882996\n",
      "Score of suitcase: 0.90630835\n",
      "Score of hair: 2.571011\n",
      "Score of high: 0.61099094\n",
      "Score of short: 0.78336006\n",
      "Score of brush: 0.9563386\n",
      "Score of comb: 0.88922715\n",
      "Score of low: 0.15\n",
      "Score of small: 0.9073383\n",
      "Score of style: 0.80175173\n",
      "Score of frill: 1.0344884\n",
      "Score of powerful: 0.8805103\n",
      "Score of dryer: 1.5065361\n",
      "Score of blow: 0.8428712\n",
      "Score of sweet: 0.78799886\n",
      "Score of time: 0.46484247\n",
      "Score of dust: 0.8462125\n",
      "Score of lightweight: 0.91369665\n",
      "Score of speed: 0.9248146\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,vocab_len):\n",
    "    print(\"Score of \"+vocabulary[i]+\": \"+str(score[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Partiotioning\n",
    "\n",
    "Paritioning lemmatized_text into phrases using the stopwords in it as delimeters.\n",
    "The phrases are also candidates for keyphrases to be extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned Phrases (Candidate Keyphrases): \n",
      "\n",
      "[['time'], ['hair', 'dryer'], ['dust'], ['year', 'small', 'lightweight'], ['suitcase'], ['powerful'], ['blow', 'dry', 'hair'], ['short', 'hair', 'sweet'], ['dryer'], ['dry', 'style', 'hair'], ['comb', 'brush', 'frill'], ['speed', 'high', 'low']]\n"
     ]
    }
   ],
   "source": [
    "phrases = []\n",
    "\n",
    "phrase = \" \"\n",
    "for word in lemmatized_text:\n",
    "    \n",
    "    if word in stopwords_plus:\n",
    "        if phrase!= \" \":\n",
    "            phrases.append(str(phrase).strip().split())\n",
    "        phrase = \" \"\n",
    "    elif word not in stopwords_plus:\n",
    "        phrase+=str(word)\n",
    "        phrase+=\" \"\n",
    "\n",
    "print(\"Partitioned Phrases (Candidate Keyphrases): \\n\")\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
