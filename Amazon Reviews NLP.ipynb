{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Louie Balderrama<br>\n",
    "Springboard Data Science Career Track, January 2019 cohort<br>\n",
    "\n",
    "<h1 align=\"center\">Capstone Project II</h1>\n",
    "\n",
    "# Introduction #\n",
    "**Problem Statement**: Classifying Amazon reviews based on customer ratings using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 align=\"center\">Impact</h4>\n",
    "\n",
    "Reviews provide objective feedback to a product and are therefore inherently useful for consumers. These ratings are often summarized by a numerical rating, or the number of stars. Of course there is more value in the actual text itself than the quantified stars. And at times, the given rating does not truly convey the experience of the product â€“ the heart of the feedback is actually in the text itself. The goal therefore is to build a classifier that would understand the essence of a piece of review and assign it the most appropriate rating based on the meaning of the text.\n",
    "\n",
    "<h4 align=\"center\">Background</h4>\n",
    "\n",
    "Though product ratings on Amazon are aggregated from all the reviews by every customer, each individual rating is actually only an integer that ranges from one star to five stars. This reduces our predictions to discrete classes totaling five possibilities. Therefore what we'll have is a supervised, multi-class classifier with the actual review text as the core predictor.\n",
    "\n",
    "This study is an exploration of Natural Language Processing (NLP). The goal of predicting the star rating given a piece of text will take on different NLP topics including word embedding, topic modeling, and dimension reduction. From there, we'll arrive at a final dataframe and we'll be employing different machine learning techniques in order to come up with the best approach (i.e. most accurate estimator) for our classifier.\n",
    "\n",
    "<h4 align=\"center\" id=\"Datasets\">Datasets</h4>\n",
    "\n",
    "The [Amazon dataset](http://jmcauley.ucsd.edu/data/amazon/index.html) contains the customer reviews for all listed *Electronics* products spanning from May 1996 up to July 2014. There are a total of 1,689,188 reviews by a total of 192,403 customers on 63,001 unique products. The data dictionary is as follows:\n",
    "\n",
    "*  **asin** - Unique ID of the product being reviewed, *string*\n",
    "*  **helpful** - A list with two elements: the number of users that voted *helpful*, and the total number of users that voted on the review (including the *not helpful* votes), *list*\n",
    "*  **overall** - The reviewer's rating of the product, *int64*\n",
    "*  **reviewText** - The review text itself, *string*\n",
    "*  **reviewerID** - Unique ID of the reviewer, *string*\n",
    "*  **reviewerName** - Specified name of the reviewer, *string*\n",
    "*  **summary** - Headline summary of the review, *string*\n",
    "*  **unixReviewTime** - Unix Time of when the review was posted, *string*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `df` is created from the Amazon dataset. If the file has been downloaded then the dataset is loaded from the local file. Otherwise the file is accessed and extracted directly from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./hair_dryer.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>34678741</td>\n",
       "      <td>R9T1FE2ZX2X04</td>\n",
       "      <td>B003V264WW</td>\n",
       "      <td>732252283</td>\n",
       "      <td>remington ac2015 t|studio salon collection pea...</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Works great</td>\n",
       "      <td>Works great!</td>\n",
       "      <td>8-31-2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>11599505</td>\n",
       "      <td>RE36JAD5V53PO</td>\n",
       "      <td>B0009XH6V4</td>\n",
       "      <td>670161917</td>\n",
       "      <td>andis micro turbo hair dryer</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>I love travel blow dryers because they are eas...</td>\n",
       "      <td>This dries my hair faster that bigger, more po...</td>\n",
       "      <td>8-31-2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>2282190</td>\n",
       "      <td>RIDHM8B7SCCV3</td>\n",
       "      <td>B0007NZPY6</td>\n",
       "      <td>16483457</td>\n",
       "      <td>conair pro hair dryer</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Love this dryer!</td>\n",
       "      <td>8-31-2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>43669858</td>\n",
       "      <td>R14QGWPCHU9LSE</td>\n",
       "      <td>B00BB8ZIW0</td>\n",
       "      <td>253917972</td>\n",
       "      <td>remington silk ceramic professional hair dryer</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>styling hair in style</td>\n",
       "      <td>8-31-2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>107098</td>\n",
       "      <td>R35BHQJHXXJD59</td>\n",
       "      <td>B003V264WW</td>\n",
       "      <td>732252283</td>\n",
       "      <td>remington ac2015 t|studio salon collection pea...</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>I think's great. The cord length is perfect</td>\n",
       "      <td>I just got this last week. I think's great. Th...</td>\n",
       "      <td>8-31-2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>US</td>\n",
       "      <td>51995766</td>\n",
       "      <td>R230LCPQDOFJJZ</td>\n",
       "      <td>B000065DJY</td>\n",
       "      <td>919751065</td>\n",
       "      <td>revlon 1875w volumizing hair dryer</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Excellent dryer.</td>\n",
       "      <td>8-31-2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>US</td>\n",
       "      <td>39431051</td>\n",
       "      <td>R21NN9ONVZITI0</td>\n",
       "      <td>B000FS1W4U</td>\n",
       "      <td>235105995</td>\n",
       "      <td>revlon essentials 1875w fast dry hair dryer, r...</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Gets extremely hot - I have burned my hand on ...</td>\n",
       "      <td>Gets extremely hot - I have burned my hand on ...</td>\n",
       "      <td>8-31-2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>US</td>\n",
       "      <td>180659</td>\n",
       "      <td>RYOOYLVIAHU2A</td>\n",
       "      <td>B003FBG88E</td>\n",
       "      <td>195677102</td>\n",
       "      <td>conair pro styler ionic conditioning hair dryer</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Everything okay but.....!!</td>\n",
       "      <td>I found everything goes well except the plug. ...</td>\n",
       "      <td>8-31-2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>US</td>\n",
       "      <td>17023782</td>\n",
       "      <td>R18NK8BQ5LPMZZ</td>\n",
       "      <td>B0057HQ6C2</td>\n",
       "      <td>582752797</td>\n",
       "      <td>pibbs ttec8012 twin turbo 3800 professional io...</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>8-31-2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>US</td>\n",
       "      <td>17563775</td>\n",
       "      <td>RD0BGSERMZ2JS</td>\n",
       "      <td>B00132ZG3U</td>\n",
       "      <td>758099411</td>\n",
       "      <td>conair 1875 watt tourmaline ceramic hair dryer</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Nice hairdryer that works very well.</td>\n",
       "      <td>I really like this hairdryer. I haven't had it...</td>\n",
       "      <td>8-31-2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     34678741   R9T1FE2ZX2X04  B003V264WW       732252283   \n",
       "1          US     11599505   RE36JAD5V53PO  B0009XH6V4       670161917   \n",
       "2          US      2282190   RIDHM8B7SCCV3  B0007NZPY6        16483457   \n",
       "3          US     43669858  R14QGWPCHU9LSE  B00BB8ZIW0       253917972   \n",
       "4          US       107098  R35BHQJHXXJD59  B003V264WW       732252283   \n",
       "5          US     51995766  R230LCPQDOFJJZ  B000065DJY       919751065   \n",
       "6          US     39431051  R21NN9ONVZITI0  B000FS1W4U       235105995   \n",
       "7          US       180659   RYOOYLVIAHU2A  B003FBG88E       195677102   \n",
       "8          US     17023782  R18NK8BQ5LPMZZ  B0057HQ6C2       582752797   \n",
       "9          US     17563775   RD0BGSERMZ2JS  B00132ZG3U       758099411   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0  remington ac2015 t|studio salon collection pea...           Beauty   \n",
       "1                       andis micro turbo hair dryer           Beauty   \n",
       "2                              conair pro hair dryer           Beauty   \n",
       "3     remington silk ceramic professional hair dryer           Beauty   \n",
       "4  remington ac2015 t|studio salon collection pea...           Beauty   \n",
       "5                 revlon 1875w volumizing hair dryer           Beauty   \n",
       "6  revlon essentials 1875w fast dry hair dryer, r...           Beauty   \n",
       "7    conair pro styler ionic conditioning hair dryer           Beauty   \n",
       "8  pibbs ttec8012 twin turbo 3800 professional io...           Beauty   \n",
       "9     conair 1875 watt tourmaline ceramic hair dryer           Beauty   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0            5              0            0    N                 Y   \n",
       "1            4              0            0    N                 Y   \n",
       "2            5              0            1    N                 Y   \n",
       "3            5              0            0    N                 Y   \n",
       "4            4              0            0    N                 N   \n",
       "5            5              0            0    N                 Y   \n",
       "6            1              0            0    N                 N   \n",
       "7            3              1            1    N                 Y   \n",
       "8            5              0            0    N                 Y   \n",
       "9            5              0            1    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                        Works great   \n",
       "1  I love travel blow dryers because they are eas...   \n",
       "2                                         Five Stars   \n",
       "3                                         Five Stars   \n",
       "4        I think's great. The cord length is perfect   \n",
       "5                                         Five Stars   \n",
       "6  Gets extremely hot - I have burned my hand on ...   \n",
       "7                         Everything okay but.....!!   \n",
       "8                                         Five Stars   \n",
       "9               Nice hairdryer that works very well.   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0                                       Works great!   8-31-2015  \n",
       "1  This dries my hair faster that bigger, more po...   8-31-2015  \n",
       "2                                   Love this dryer!   8-31-2015  \n",
       "3                              styling hair in style   8-31-2015  \n",
       "4  I just got this last week. I think's great. Th...   8-31-2015  \n",
       "5                                   Excellent dryer.   8-31-2015  \n",
       "6  Gets extremely hot - I have burned my hand on ...   8-31-2015  \n",
       "7  I found everything goes well except the plug. ...   8-31-2015  \n",
       "8                                            Perfect   8-31-2015  \n",
       "9  I really like this hairdryer. I haven't had it...   8-31-2015  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# dataset = \"Electronics_5.json\"\n",
    "\n",
    "# if os.path.isfile(dataset):\n",
    "#     df = pd.read_json(\"Electronics_5.json\", lines=True)\n",
    "# else:\n",
    "#     url = r\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz\"\n",
    "#     df = pd.read_json(url, compression='gzip', lines=True)\n",
    "\n",
    "tab1 = \"./hair_dryer.tsv\"\n",
    "print(tab1)\n",
    "tab2 = \".\"\n",
    "df = pd.read_csv(tab1, sep='\\t', header=0)\n",
    "\n",
    "\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the `overall` and the `unixReviewTime` series are stored as integers. The rest are interpreted as strings (objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11470 entries, 0 to 11469\n",
      "Data columns (total 15 columns):\n",
      "marketplace          11470 non-null object\n",
      "customer_id          11470 non-null int64\n",
      "review_id            11470 non-null object\n",
      "product_id           11470 non-null object\n",
      "product_parent       11470 non-null int64\n",
      "product_title        11470 non-null object\n",
      "product_category     11470 non-null object\n",
      "star_rating          11470 non-null int64\n",
      "helpful_votes        11470 non-null int64\n",
      "total_votes          11470 non-null int64\n",
      "vine                 11470 non-null object\n",
      "verified_purchase    11470 non-null object\n",
      "review_headline      11468 non-null object\n",
      "review_body          11470 non-null object\n",
      "review_date          11463 non-null object\n",
      "dtypes: int64(5), object(10)\n",
      "memory usage: 1.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each review is stored as string in the `review_body` series. A sample product review is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Works great!\n"
     ]
    }
   ],
   "source": [
    "print(df[\"review_body\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each review is associated with a rating stored under the `overall` field. This serves as the quantified summary of a given review and will thus be used as the ground truth labels for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 4 1 3 2]\n"
     ]
    }
   ],
   "source": [
    "print(df.star_rating.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pre-Processing ##\n",
    "We'll work with `review_body` to prepare our model's final dataframe. The goal is to produce tokens for every document (i.e. every review). These documents will make up our corpora where we'll draw our vocabulary from.\n",
    "\n",
    "The following is a sample text in its original form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My first Yellow Bird lasted over 7 years and plenty of use by myself and friends. The best dryer ever. This new one works just as well, though I don't think I'll loan this one out so it might last longer.\n"
     ]
    }
   ],
   "source": [
    "sample_review = df[\"review_body\"].iloc[3365]\n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Entities ###\n",
    "Some special characters like the apostrophe (â€™) and the en dash (â€“) are expressed as a set of numbers prefixed by `&#` and suffixed by `;`. This is because the dataset was scraped from an HTML parser, and the dataset itself includes data that predated the universal UTF-8 standard.\n",
    "\n",
    "These *HTML Entities* can be decoded by importing the `html` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My first Yellow Bird lasted over 7 years and plenty of use by myself and friends. The best dryer ever. This new one works just as well, though I don't think I'll loan this one out so it might last longer.\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "decoded_review = html.unescape(sample_review)\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since punctuation marks do not add value in the way we'll perform NLP, all the HTML entities in the review texts can be dropped. The output series `preprocessed` is our `reviewText` but without the special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My first Yellow Bird lasted over 7 years and plenty of use by myself and friends. The best dryer ever. This new one works just as well, though I don't think I'll loan this one out so it might last longer.\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"\\&\\#[0-9]+\\;\"\n",
    "\n",
    "df[\"preprocessed\"] = df[\"review_body\"].str.replace(pat=pattern, repl=\"\", regex=True)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[3365])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the root word ###\n",
    "How often a word is used is key information in natural language processing. It is therefore important to reduce words to their root form. An example would be the usage of the word \"*learn*\". If we differentiate this base form from a modified version like \"*learning*\" then we might lose relational context between two documents that have used either word.\n",
    "\n",
    "We'll be using Lemmatization to reduce tokens to their base word. This technique takes into account context similarity according to part-of-speech anatomy. Stemming is another common approach, although stemming only performs truncation and would not be able to reduce \"*taught*\" to \"*teach*\".\n",
    "\n",
    "We will be using the *WordNetLemmatizer* from the Natural Language Toolkit (or *NLTK*). Lemmatization only applies to each word but it is dependent on sentence structure to understand context. We therefore need to have part-of-speech tags associated with each word. Our output is derived from applying the `lemmatize_doc` function to our `preprocessed` column.\n",
    "\n",
    "The `lemmatize_doc` works as follows:\n",
    "* Each review is broken down into a list of sentences\n",
    "* Punctuations that only group words or separate sentences (hyphens therefore are excluded) are removed (replaced by whitespace) using RegEx\n",
    "* Every sentence is further broken down into words (tokens)\n",
    "\n",
    "Each of the sentences then becomes an ordered bag of words. Every word is then *tagged* to a part-of-speech. This word-tag tuple pair is then fed one at a time to the `lemmatize_word` function, which works as follows:\n",
    "* Only modifiable words â€“ nouns, verbs, adjectives, and adverbs â€“ can be reduced to roots\n",
    "* These words are lemmatized and appended to the `root` list\n",
    "* Words that are not modifiable are added as they are to the `root` list\n",
    "\n",
    "The output lists are linked together as a string using whitespace. In the end, each `preprocessed` review will retain its text form but with each word simplified as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/alphonse/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alphonse/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/alphonse/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/alphonse/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#import nltk resources\n",
    "resources = [\"wordnet\", \"stopwords\", \"punkt\", \\\n",
    "             \"averaged_perceptron_tagger\", \"maxent_treebank_pos_tagger\"]\n",
    "\n",
    "for resource in resources:\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/\" + resource)\n",
    "    except LookupError:\n",
    "        nltk.download(resource)\n",
    "\n",
    "#create Lemmatizer object\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_word(tagged_token):\n",
    "    \"\"\" Returns lemmatized word given its tag\"\"\"\n",
    "    root = []\n",
    "    for token in tagged_token:\n",
    "        tag = token[1][0]\n",
    "        word = token[0]\n",
    "        if tag.startswith('J'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADJ))\n",
    "        elif tag.startswith('V'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.VERB))\n",
    "        elif tag.startswith('N'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.NOUN))\n",
    "        elif tag.startswith('R'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADV))\n",
    "        else:          \n",
    "            root.append(word)\n",
    "    return root\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_doc(document):\n",
    "    \"\"\" Tags words then returns sentence with lemmatized words\"\"\"\n",
    "    lemmatized_list = []\n",
    "    tokenized_sent = sent_tokenize(document)\n",
    "    for sentence in tokenized_sent:\n",
    "        no_punctuation = re.sub(r\"[`'\\\",.!?()]\", \" \", sentence)\n",
    "        tokenized_word = word_tokenize(no_punctuation)\n",
    "        tagged_token = pos_tag(tokenized_word)\n",
    "        lemmatized = lemmatize_word(tagged_token)\n",
    "        lemmatized_list.extend(lemmatized)\n",
    "    return \" \".join(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My first Yellow Bird last over 7 year and plenty of use by myself and friend The best dryer ever This new one work just as well though I don t think I ll loan this one out so it might last longer\n"
     ]
    }
   ],
   "source": [
    "#apply our functions\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].apply(lambda row: lemmatize_doc(row))\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[3365])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Accents ###\n",
    "Each review is normalized from longform UTF-8 to ASCII encoding. This will remove accents in characters and ensure that words like \"*naÃ¯ve*\" will simply be interpreted as (and therefore not differentiated from) \"*naive*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My first Yellow Bird last over 7 year and plenty of use by myself and friend The best dryer ever This new one work just as well though I don t think I ll loan this one out so it might last longer\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "remove_accent = lambda text: normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].apply(remove_accent)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[3365])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuations ###\n",
    "The `preprocessed` reviews are further cleaned by dropping punctuations. Using regular expressions, only whitespaces and alphanumeric characters are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My first Yellow Bird last over 7 year and plenty of use by myself and friend The best dryer ever This new one work just as well though I don t think I ll loan this one out so it might last longer\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].str.replace(pat=pattern, repl=\" \", regex=True)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[3365])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Lower Case ###\n",
    "Every letter is also converted to lower case. This makes it so that \"*iPhone*\" will not be distinguishable from \"*iphone*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my first yellow bird last over 7 year and plenty of use by myself and friend the best dryer ever this new one work just as well though i don t think i ll loan this one out so it might last longer\n"
     ]
    }
   ],
   "source": [
    "df[\"preprocessed\"] = df[\"preprocessed\"].str.lower()\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[3365])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words ###\n",
    "Stop words consist of the most commonly used words that include pronouns (e.g. *us*, *she*, *their*), articles (e.g. *the*), and prepositions (e.g. *under*, *from*, *off*). These words are not helpful in distinguishing a document from another and are therefore dropped.\n",
    "\n",
    "Note that the `stop_words` were stripped of punctuations just as what we have done to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample stop words: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', 'youll', 'youd', 'your', 'yours'] \n",
      "\n",
      "first yellow bird last 7 year plenty use friend best dryer ever new one work well though think loan one might last longer\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "stop_words = [word.replace(\"\\'\", \"\") for word in stop_words]\n",
    "\n",
    "print(f\"sample stop words: {stop_words[:15]} \\n\")\n",
    "\n",
    "remove_stop_words = lambda row: \" \".join([token for token in row.split(\" \") \\\n",
    "                                          if token not in stop_words])\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].apply(remove_stop_words)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[3365])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Extra Spaces ###\n",
    "Again, we make use of regular expressions to ensure we never get more than a single whitespace to separate words in our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first yellow bird last 7 year plenty use friend best dryer ever new one work well though think loan one might last longer\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"[\\s]+\"\n",
    "\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].str.replace(pat=pattern, repl=\" \", regex=True)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[3365])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization ##\n",
    "The entries for the `preprocessed` column are extracted to make up our *corpora*, which is simply a collection of all our documents. Each review is then transformed into an ordered list of words. This is the process of *tokenization* â€“ the document is broken down into individual words or tokens.\n",
    "\n",
    "Our tokenized sample review is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'yellow', 'bird', 'last', '7', 'year', 'plenty', 'use', 'friend', 'best', 'dryer', 'ever', 'new', 'one', 'work', 'well', 'though', 'think', 'loan', 'one', 'might', 'last', 'longer']\n"
     ]
    }
   ],
   "source": [
    "corpora = df[\"preprocessed\"].values\n",
    "tokenized = [corpus.split(\" \") for corpus in corpora]\n",
    "\n",
    "print(tokenized[3365])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Modeling ##\n",
    "Since order of words matter in most NLP models, it is often helpful to group neighboring words that appear to convey one meaning as though they are a single word, like *smart TV*.\n",
    "\n",
    "To be considered a *phrase*, the number of times that two words should appear next to each other is set to at least `300`. The *threshold* then takes that minimum and compares it to the total number of token instances in the corpora. The higher the threshold, the more often two words must appear adjacent to be grouped into a phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "bi_gram = Phrases(tokenized, min_count=300, threshold=50)\n",
    "\n",
    "tri_gram = Phrases(bi_gram[tokenized], min_count=300, threshold=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams ###\n",
    "Unigrams are single pieces of tokens. The code below takes all the unique words from the entire corpora and prints a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cradle', 'dreadlock', 'rid', 'expectation', '30', 'hotels', '198', 'rotary', 'retired', 'dependable', 'ilghtweight', 'cabnit', 'extortion', 'quibble', 'beware', 'dense', 'vase', 'beast', 'goop', 'starting', 'fizzies', 'vented', 'greasy', 'condition', '85', '2004', 'marble', 'majority', 'thursday', 'gfci', 'hiow', 'fineness', 'ms', 'spends', 'oxidize', 'frustration', 'silvery', 'mot', 'loudness', 'forced', 'decision', 'singed', '000', 'conventional', 'tames', '235', 'traditionally', 'carnival', 'modern', 'konk']\n",
      "10386\n"
     ]
    }
   ],
   "source": [
    "uni_gram_tokens = set([token for text in tokenized for token in text])\n",
    "uni_gram_tokens = set(filter(lambda x: x != \"\", uni_gram_tokens))\n",
    "\n",
    "print(list(uni_gram_tokens)[:50])\n",
    "print(len(list(uni_gram_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams ###\n",
    "Bigrams are generated from using the *gensim* phraser. Only those that pass the `bi_gram` criteria are considered.\n",
    "\n",
    "The code below takes all the unique bigram phrases from the entire corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blow_dryer', 'long_hair', 'br_br', 'hair_dry', 'work_great', 'dry_hair', 'work_well', 'hair_dryer', 'blow_dry', 'dryer_work', 'light_weight', 'heat_setting', 'retractable_cord', 'thick_hair', 'hair_quickly']\n"
     ]
    }
   ],
   "source": [
    "bigram_min = bi_gram.min_count\n",
    "\n",
    "bi_condition = lambda x: x[1] >= bigram_min\n",
    "\n",
    "bi_gram_tokens = dict(filter(bi_condition, bi_gram.vocab.items()))\n",
    "bi_gram_tokens = set([token.decode(\"utf-8\") \\\n",
    "                      for token in bi_gram_tokens])\n",
    "\n",
    "bi_grams_only = bi_gram_tokens.difference(uni_gram_tokens)\n",
    "print(list(bi_grams_only)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams ###\n",
    "Trigrams are generated by applying another *gensim* phraser on top of a bigram phraser. Take for example the tokens *sd* and *card*. Because they appear often together enough, they become linked together as *sd_card*. In turn, if *sd_card* appears adjacent to the token *reader* in enough instances, then the `tri_gram` model would link them together as well to tokenize *sd_card_reader*.\n",
    "\n",
    "The code below takes all the unique trigram phrases from the entire corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "trigram_min = tri_gram.min_count\n",
    "\n",
    "tri_condition = lambda x: x[1] >= trigram_min\n",
    "\n",
    "tri_gram_tokens = dict(filter(tri_condition, tri_gram.vocab.items()))\n",
    "tri_gram_tokens = set([token.decode(\"utf-8\") \\\n",
    "                       for token in tri_gram_tokens])\n",
    "\n",
    "tri_grams_only = tri_gram_tokens.difference(bi_gram_tokens)\n",
    "print(list(tri_grams_only)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tri_gram` and `bi_gram` phrasers are applied to our `tokenized` corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized = [Phraser(tri_gram)[Phraser(bi_gram)[i]] for i in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single-character tokens are removed from every tokenized document. Our tokenized review, in its final form, is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [list(filter(lambda x: len(x) > 1, document)) \\\n",
    "             for document in tokenized]\n",
    "\n",
    "print(tokenized[3365])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Vocabulary ##\n",
    "The `vocabulary` is the key-value pairs of all the unique tokens from every product review. Each token is assigned a lookup ID. The first 10 words in our dictionary are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "vocabulary = Dictionary(tokenized)\n",
    "\n",
    "vocabulary_keys = list(vocabulary.token2id)[0:10]\n",
    "\n",
    "for key in vocabulary_keys:\n",
    "    print(f\"ID: {vocabulary.token2id[key]}, Token: {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count-based Feature Engineering ##\n",
    "In order for a machine learning model to work with text input, the document must first be *vectorized*. This simply means that the input has to be converted into containers of numerical values.\n",
    "\n",
    "### Bag of Words Model ###\n",
    "The classical approach in expressing text as a set of features is getting the token frequency. Each entry to the dataframe is a document while each column corresponds to every unique token in the entire corpora. The row will identify how many times a word appears in the document. The `bow` model for the sample review is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = [vocabulary.doc2bow(doc) for doc in tokenized]\n",
    "\n",
    "for idx, freq in bow[0]:\n",
    "    print(f\"Word: {vocabulary.get(idx)}, Frequency: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Model ###\n",
    "The Term Frequency-Inverse Document Frequency (*TF-IDF*) approach assigns continuous values instead of simple integers for the token frequency. Words that appear frequently overall tend to not establish saliency in a document, and are thus weighted lower. Words that are unique to some documents tend to help distinguish it from the rest and are thus weighted higher. The `tfidf` weighting is based on our `bow` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "tfidf = TfidfModel(bow)\n",
    "\n",
    "for idx, weight in tfidf[bow[0]]:\n",
    "    print(f\"Word: {vocabulary.get(idx)}, Weight: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding for Feature Engineering ##\n",
    "The downside of count-based techniques is that without regard to word sequence and sentence structure, the semantics get lost. The *Word2Vec* technique, on the other hand, actually embeds meaning in vectors by quantifying how often a word appears within the vicinity of a given set of other words.\n",
    "\n",
    "A context window the span of `context_size` slides across every document one token at a time. In each step, the center word is described by its adjacent words and the probability that the token appears together with the others is expressed in `feature_size` dimensions. Since the minimum word requirement is set to `1`, every token in the corpora is embedded in the *Word2Vec* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "feature_size = 100\n",
    "context_size = 20\n",
    "min_word = 1\n",
    "\n",
    "word_vec= word2vec.Word2Vec(tokenized, size=feature_size, \\\n",
    "                            window=context_size, min_count=min_word, \\\n",
    "                            iter=50, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Dataframe ##\n",
    "The goal is to have a dataframe with observations corresponding to the product reviews. The `word_vec` model is used to gather all the unique tokens in the corpora. This enables us to generate the `word_vec_df` which makes use of the dimensions as the features of every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_unpack = [(word, idx.index) for word, idx in \\\n",
    "                   word_vec.wv.vocab.items()]\n",
    "\n",
    "tokens, indexes = zip(*word_vec_unpack)\n",
    "\n",
    "word_vec_df = pd.DataFrame(word_vec.wv.syn0[indexes, :], index=tokens)\n",
    "\n",
    "display(word_vec_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `word_vec_df` is sliced by the words that appear in a given `tokenized` review and the mean along every dimension is taken. The resulting `model_array` shape is therefore the word count on *axis 0* and the number of dimensions on *axis 1*. This singularizes multiple word embeddings into one observation for each review.\n",
    "\n",
    "If multiple occurrences of a word occurs in a review, then this only emphasizes the token since the row is pulled towards the values of the vectors of that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenized_array = np.array(tokenized)\n",
    "\n",
    "model_array = np.array([word_vec_df.loc[doc].mean(axis=0) for doc in tokenized_array])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every document is provided the ground truth label by imposing its `overall` rating. This completes our finalized `model_df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_df = pd.DataFrame(model_array)\n",
    "model_df[\"label\"] = df[\"overall\"]\n",
    "\n",
    "display(model_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis ##\n",
    "Principal Component Analysis (*PCA*) is a dimensionality reduction technique that we can use on our `model_df` to reduce its 100 dimensions to just two dimensions. This will help visualize if there is a clear decision boundary along the five `overall` rating classifications. The more datapoints belonging to the same class are clustered together, the higher the likelihood that our machine learning model is simpler and more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#sampling the model_df population\n",
    "pca_df = model_df.reset_index()\n",
    "pca_df = model_df.dropna(axis=0).iloc[:,1:]\n",
    "pca_df = pca_df.iloc[::50]\n",
    "\n",
    "#setting up PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca = pca.fit_transform(pca_df.iloc[:, :-1])\n",
    "labels = pca_df[\"label\"]\n",
    "\n",
    "#setting up plot components\n",
    "x_axis = pca[:,0]\n",
    "y_axis = pca[:,1]\n",
    "color_map = pca_df[\"label\"].map({1:\"blue\", \\\n",
    "                                 2:\"red\", \\\n",
    "                                 3:\"yellow\", \\\n",
    "                                 4:\"green\", \\\n",
    "                                 5:\"orange\"})\n",
    "\n",
    "#plotting PCA\n",
    "f, axes = plt.subplots(figsize=(20,10))\n",
    "plt.scatter(x_axis, y_axis, color=color_map, s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis #\n",
    "We'll implement several interesting Natural Language Processing techniques in order to explore our Amazon dataset.\n",
    "\n",
    "## More on Word2Vec ##\n",
    "To better appreciate the concept of word embeddings, we take five common words in our corpora and derive their five most related words using our `word_vec` model. The similarity comes from how often these tokens appear in the same window of words as their `word_bank` counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_bank = [\"nook\", \"phone\", \"tv\", \"good\", \"price\"]\n",
    "\n",
    "for word in word_bank[:]:\n",
    "    related_vec = word_vec.wv.most_similar(word, topn=5)\n",
    "    related_words = np.array(related_vec)[:,0]\n",
    "    word_bank.extend(related_words)\n",
    "    print(f\"{word}: {related_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE ###\n",
    "Like PCA, the t-Distributed Stochastic Neighbor Embedding (*t-SNE*) is another dimensionality reduction technique that assists in visualizing high-dimensional datasets. To perceive the similarity between the related words in terms of spatial distance, t-SNE provided the coordinates of each word in a 2D scatterplot plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=5, n_iter=1000, random_state=42)\n",
    "\n",
    "sample_vecs = word_vec.wv[set(word_bank)]\n",
    "sample_tsne = tsne.fit_transform(sample_vecs)\n",
    "tsne_x = sample_tsne[:, 0]\n",
    "tsne_y = sample_tsne[:, 1]\n",
    "\n",
    "f, axes = plt.subplots(figsize=(20,7))\n",
    "ax = plt.scatter(x=tsne_x, y=tsne_y)\n",
    "\n",
    "for label, x, y in zip(word_bank, tsne_x, tsne_y):\n",
    "    plt.annotate(label, xy=(x+3, y+3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Algebra ###\n",
    "Since *Word2Vec* characterizes words into quantified tokens, we can consequently add or subtract word vectors together. To add is to combine the meaning of the components and to subtract is to take out the context of one token from another. The following are examples of this vector algebra and their similarity scores:\n",
    "\n",
    "**Books + Touchscreen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec.wv.most_similar(positive=[\"books\", \"touchscreen\"], \\\n",
    "                      negative=[], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cheap â€“ Quality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec.wv.most_similar(positive=[\"cheap\"], \\\n",
    "                      negative=[\"quality\"], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tablet â€“ Phone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec.wv.most_similar(positive=[\"tablet\"], \\\n",
    "                      negative=[\"phone\"], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named-Entity Recognition ###\n",
    "We've seen *gensim* perform word tagging to identify part-of-speech. Now we use *spaCy* to go further and identify what nouns in the documents refer to. Some Named-Entity Recognition (*NER*) classification tags include distinguishing persons, organizations, products, places, dates, etc.\n",
    "\n",
    "In exploring *spaCy*, we'll be using the `most_helpful_text`, which is the highest-rated product review by Amazon users. The `helpful` series from the `df` dataframe is actually a list with its first element storing the number of *helpful* votes a review received, and the second element containing the total number of *helpful* and *not helpful* review votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "helpful = df[\"helpful\"].tolist()\n",
    "most_helpful = max(helpful, key=lambda x: x[0])\n",
    "\n",
    "most_helpful_idx = df[\"helpful\"].astype(str) == str(most_helpful)\n",
    "most_helpful_idx = df[most_helpful_idx].index\n",
    "\n",
    "most_helpful_text = df[\"reviewText\"].iloc[most_helpful_idx].values[0]\n",
    "\n",
    "print(most_helpful_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `ner_dict`, a dictionary initialized as a list, to segregate the nouns in the `most_helpful_text` into the NER tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import spacy\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "ner = spacy.load(\"en\")\n",
    "\n",
    "ner_helpful = ner(most_helpful_text)\n",
    "\n",
    "ner_dict = defaultdict(list)\n",
    "for entity in ner_helpful.ents:\n",
    "    ner_dict[entity.label_].append(entity)\n",
    "\n",
    "for NER, name in ner_dict.items():\n",
    "    print(f\"{NER}:\\n{name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `displaCy` to visualize the tags in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(ner_helpful, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Tree ##\n",
    "The capability of *spaCy'*s NER is based on deciphering the structure of the sentence by breaking down how tokens interact with and influence each other. Below is the dependency trees of the first three sentences of the `most_helpful_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_displacy(sentence):\n",
    "    ner_sentence = ner(sentence)\n",
    "    displacy.render(ner_sentence, jupyter=True, \\\n",
    "                    options={\"compact\": False, \\\n",
    "                             \"distance\": 90, \\\n",
    "                             \"word_spacing\":20, \\\n",
    "                             \"arrow_spacing\":10, \\\n",
    "                             \"arrow_stroke\": 2, \\\n",
    "                             \"arrow_width\": 5})\n",
    "\n",
    "for sentence in most_helpful_text.split(\".\")[0:3]:\n",
    "    ner_displacy(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Latent Dirichlet Allocation (*LDA*) can cluster documents together according to topic, the reviews can be classified and grouped according to the type of electronics product they correspond to. The product reviews will have weights assigned to each of the topic and the topics themselves will have weights on every token. As it is a clustering-based model, LDA is unsupervised and only the `num_topics` is configurable.\n",
    "\n",
    "The following are the top five words that are salient to the first group of product reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "num_topics = 10\n",
    "bow_lda = LdaMulticore(bow, num_topics=num_topics, id2word=vocabulary, \\\n",
    "                       passes=5, workers=cores, random_state=42)\n",
    "\n",
    "for token, frequency in bow_lda.show_topic(0, topn=5):\n",
    "    print(token, frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that are the most characteristic of the topics are indeed thematic. And each word group do conjure a distinct topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in range(0, num_topics):\n",
    "    print(f\"\\nTopic {topic+1}:\")\n",
    "    for token, frequency in bow_lda.show_topic(topic, topn=5):\n",
    "        print(f\" {token}, {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using *pyLDAvis*, we can interactively explore the words associated with the topics derived by LDA. The Intertopic Distance Map shows how some product reviews in one topic converge with others due to similarity. If needed, we can adjust the `num_topics` accordingly to cluster together topic intersections so a more evident decision boundary between classes can be established."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "lda_idm = pyLDAvis.gensim.prepare(bow_lda, bow, vocabulary)\n",
    "\n",
    "pyLDAvis.display(lda_idm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning #\n",
    "We'll further process our finalized dataframe in order to make it compatible and easy to pipe into our Machine Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with NaNs ##\n",
    "It is important that we impute NaN values before we feed them into a model because machine learning algorithms can only work with *real* numbers. Our dataframe was derived from employing a *Word2Vec* model and so the only way we could have invalid entries that would become NaN values is when we have empty documents.\n",
    "\n",
    "If a review contains no tokens then every dimension would become NaN. And so to find out the indices of NaN documents, we just have to filter reviews that have a NaN on the first dimension (or any dimension at all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_list = model_df[model_df[0].isna()].index\n",
    "nan_list = nan_list.tolist()\n",
    "\n",
    "print(nan_list[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, inspecting these documents brings us empty lists which tell us that there are no tokens in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*[tokenized[blank] for blank in nan_list[0:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imposing these indices to our `df`, we can extract what these reviews originally looked like before tokenization and before all the pre-processing steps were performed. We see that, other than blanks, reviews that would become NaNs contain only minimal characters. The fourth entry is invalidated because in our steps, we have dropped all characters that are not alphanumeric leaving us with just the letter *A*. We have also chosen in our pre-processing that single-characters would not be tokenized. The fourth review would therefore end up as an empty list after our NLP steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for blank in nan_list[0:5]:\n",
    "    display(df[\"reviewText\"].iloc[blank])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model_df` is updated by dropping the NaN documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Original 'model_df' count: {len(model_df)}\")\n",
    "print(f\"Final 'model_df' count: {len(model_df.dropna(axis=0))}\")\n",
    "\n",
    "model_df = model_df.dropna(axis=0)\n",
    "display(model_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Unbalanced Data ##\n",
    "The distribution of ratings shows that, in general, users highly approve of products bought on Amazon. This however gives us a highly imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "f, axes = plt.subplots(figsize=(20,7))\n",
    "ax = sns.countplot(x=df[\"overall\"], palette=\"OrRd_r\")\n",
    "ax.set(title=\"Distribution of Product Ratings\", \\\n",
    "       xlabel=\"Rating\", ylabel=\"Number of Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model simply classified every review as `5`, then an accuracy of around 60% can be achieved given this exact dataset. Since this would outperform predictions made by chance, we should therefore ensure that we stratify the testing set where we base the final score of the model.\n",
    "\n",
    "To deal with this we will have to take into account underrepresenting the majority and/or overrepresenting the minority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "majority = df[\"overall\"] == 5\n",
    "majority_ratio = len(df[majority]) / len(df)\n",
    "\n",
    "print(f\"{majority_ratio*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underrepresentation vs. Overrepresentation ##\n",
    "\n",
    "Performing over-representation is possible by bootstrapping the minority classes to match the size of the majority classes. This can be done using K-Nearest Neighbors (*KNN*) or via Support Vector Machine (*SVM*) by clustering a given class first before generating random samples within the decision boundaries of the class. A popular module called `SMOTE`, or *Synthetic Minority Over-sampling Technique*, does exactly this. However, since the imbalance in our classes is massive, and because we have 100 dimensions for each one of our almost 1.7 million observations, this approach is extremely computationally expensive.\n",
    "\n",
    "Because our dataset is huge, we can afford to perform sampling in every class and still have a significant amount of data for the model. This way, we can then opt to *underrepresent* the majority class according to our most minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = len(model_df[model_df[\"label\"] == 2])\n",
    "print(f\"Size of the most underrepresented class: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In choosing this route to deal with imbalance, we create a trimmed version of our dataframe, `trimmed_df`. Each class is trimmed to have the same number of entries as the smallest class which is *Class 2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trim the majority class\n",
    "condition = model_df[\"label\"] == 5\n",
    "trimmed_df = model_df[condition].sample(n=count, random_state=42)\n",
    "\n",
    "#trim other class and add on to the trimmed_df\n",
    "for rating in [1, 2, 3, 4]:\n",
    "    condition = model_df[\"label\"] == rating\n",
    "    if len(model_df[condition]) >= count:\n",
    "        add_df = model_df[condition].sample(n=count, random_state=42)\n",
    "    else:\n",
    "        add_df = model_df[condition]\n",
    "    trimmed_df = pd.concat([trimmed_df, add_df], ignore_index=False)\n",
    "\n",
    "#display new class sizes of trimmed_df\n",
    "for rating in [1, 2, 3, 4, 5]:\n",
    "    class_size = len(trimmed_df[trimmed_df[\"label\"] == rating])\n",
    "    print(f\"Size of Class {rating}: {class_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `trimmed_df` is arranged by class from 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trimmed_df = trimmed_df.sort_values(by=\"label\")\n",
    "display(trimmed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we now have a perfectly balanced dataset after we performed underrepresentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(figsize=(20,7))\n",
    "ax = sns.countplot(x=trimmed_df[\"label\"], palette=\"OrRd_r\")\n",
    "ax.set(title=\"Distribution of Product Ratings after Underrepresentation\", \\\n",
    "       xlabel=\"Rating\", ylabel=\"Number of Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split ##\n",
    "The `y` is our target variable or the labels for the data. The `X` constitutes the features and are the predictor variables.\n",
    "\n",
    "We evenly split the training and testing sets and *stratify* to ensure the ratio of classes in both sets are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = trimmed_df.iloc[:, :-1]\n",
    "y = trimmed_df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring and Baseline ##\n",
    "In our study, we will make use of two metrics to measure the model performance:\n",
    "* Accuracy\n",
    "* F1 Score\n",
    "\n",
    "Accuracy will identify how many reviews are correctly labeled by the model. There are five ratings and thus five classes. No review can have two or more ratings and so the probability that a correct prediction is made from pure guesswork is `20%`.\n",
    "\n",
    "The F1 score is taking *precision* and *recall* into consideration. Taking into account false positives and false negatives for each class is especially important in inherently imbalanced datasets.\n",
    "\n",
    "The baseline scores below are for when a model only randomly guesses the output labels â€“ in this case, when every prediction is the same class. The scores are also based on an evenly distributed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "label_shape = np.shape(y_test)\n",
    "y_baseline = np.full(label_shape, 5)\n",
    "\n",
    "accuracy_baseline = metrics.accuracy_score(y_test, y_baseline)\n",
    "f1_score_baseline = metrics.f1_score(y_test, y_baseline, average=\"micro\")\n",
    "\n",
    "print(f\"Baseline Accuracy: {accuracy_baseline*100:.3f}%\")\n",
    "print(f\"Baseline F1 Score: {f1_score_baseline:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest ##\n",
    "Random Forest actually has a native way of supporting datasets that have class imbalance. We will therefore be able to use the original `model_df` instead of the sample `trimmed_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = model_df.iloc[:, :-1]\n",
    "y = model_df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `class_weight` attribute is provided with a dictionary that represents the associated weight of each class â€“ the majority class is given a *1* and the rest are given the multiplying factor at which they would level with the largest class.\n",
    "\n",
    "The criteria chosen is `entropy` which is similar to `gini` but instead of splitting nodes until there are pure classes, the nodes are split until the classes within have equal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=25, class_weight={1:10, 2:12, 3:7, 4:9, 5:1}, \\\n",
    "                                criterion=\"entropy\", random_state=42)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tuned Random Forest model got a very high score on the training data. The confusion matrix plotted below highlighted how the model almost perfectly classified each Amazon review accordingly.\n",
    "\n",
    "However, these scores may be misleading since they are based on the data that the model were trained on. This is highly likely a result of *overfitting*. It is then important to rate our model more effectively without digging into our reserved test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forest.predict(X_train)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_train, y_pred)\n",
    "f1_score = metrics.f1_score(y_train, y_pred, average=\"micro\")\n",
    "\n",
    "print(f\"Training Set Accuracy: {accuracy*100:.3f}%\")\n",
    "print(f\"Training Set F1 Score: {f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#create the confusion matrix of the training set\n",
    "confusion_train = confusion_matrix(y_train, y_pred)\n",
    "confusion_train = confusion_train.astype(\"float\") / \\\n",
    "                   confusion_train.sum(axis=1)[:, np.newaxis]\n",
    "confusion_train = np.around(confusion_train, decimals=3)*100\n",
    "\n",
    "#create confusion matrix heat map\n",
    "f, axes = plt.subplots(figsize=(20,10))\n",
    "im = axes.imshow(confusion_train, interpolation=\"nearest\", cmap=plt.cm.Reds)\n",
    "\n",
    "axes.figure.colorbar(im, ax=axes)\n",
    "axes.set(title=\"Confusion Matrix for Training Set\", \\\n",
    "         xticks=np.arange(confusion_train.shape[1]), \\\n",
    "         yticks=np.arange(confusion_train.shape[0]), \\\n",
    "         xticklabels=range(1, 6), yticklabels=range(1, 6), \\\n",
    "         xlabel=\"Predicted\", ylabel=\"Truth\")\n",
    "\n",
    "#add clear annotations to the confusion matrix\n",
    "threshold = confusion_train.max()/1.5\n",
    "for i in range(confusion_train.shape[0]):\n",
    "    for j in range(confusion_train.shape[1]):\n",
    "        axes.text(j, i, f\"{confusion_train[i, j]:.3f}%\",\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if confusion_train[i, j] > threshold else \"black\")\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation ##\n",
    "Cross-validation makes the most of the training data by splitting the training set into *folds* and further subjecting each fold to train-test splits. Cross-validation can thus test against overfitting and the resulting scores can better reflect how the model performs on data it has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_accuracy = cross_val_score(forest, X_train, y_train, \\\n",
    "                               cv=3, scoring=\"accuracy\")\n",
    "cross_val_f1 = cross_val_score(forest, X_train, y_train, \\\n",
    "                               cv=3, scoring=\"f1_micro\")\n",
    "\n",
    "cross_val_accuracy = np.mean(cross_val_accuracy)\n",
    "cross_val_f1 = np.mean(cross_val_f1)\n",
    "\n",
    "print(f\"Training Set Accuracy: {cross_val_accuracy*100:.3f}%\")\n",
    "print(f\"Training Set F1 Score: {cross_val_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to create a model based on a popular boosting technique and see how it compares with our Random Forest model (which is a tree-based bagging approach). XGBoost has become a staple in Kaggle competitions because of its high rate of success and its ease-of-use.\n",
    "\n",
    "The class notation for our *XGBoost* object `boost` begins from 0, and so we perform an element-wise shift of our labels *from 1 to 0*, *from 2 to 1*, *from 3 to 2*, etc. We tune our model using the maximum number of depths, the learning rate (*eta*), the number of classes, etc. We expect our outputs to be multi-class and so we select `softprob` as our *objective*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import xgboost as xgb\n",
    "\n",
    "y_train_shifted = y_train-1\n",
    "y_test_shifted = y_test-1\n",
    "\n",
    "train_set = xgb.DMatrix(X_train, label=y_train_shifted)\n",
    "test_set = xgb.DMatrix(X_test, label=y_test_shifted)\n",
    "\n",
    "parameters = {\"max_depth\": 10, \"eta\": 0.2, \"silent\": 1, \\\n",
    "              \"objective\": \"multi:softprob\", \"num_class\": 5}\n",
    "\n",
    "boost = xgb.train(parameters, train_set, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array of predicted labels `y_pred` contains lists of probabilities for each class per product review. The class that is deemed most likely is chosen by the *argmax* and the labels are shifted back to their original state.\n",
    "\n",
    "The `micro` approach in averaging the F1 score means that the false positives, true positives, and false negatives are taken into account across all classes. This is in contrast with the `macro` approach that instead averages the F1 scores of each class independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = boost.predict(train_set)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "y_pred = y_pred+1\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_train, y_pred)\n",
    "f1_score = metrics.f1_score(y_train, y_pred, average=\"micro\")\n",
    "\n",
    "print(f\"Training Set Accuracy: {accuracy*100:.3f}%\")\n",
    "print(f\"Training Set F1 Score: {f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the confusion matrix of the training set\n",
    "confusion_train = confusion_matrix(y_train, y_pred)\n",
    "confusion_train = confusion_train.astype(\"float\") / \\\n",
    "                   confusion_train.sum(axis=1)[:, np.newaxis]\n",
    "confusion_train = np.around(confusion_train, decimals=3)*100\n",
    "\n",
    "#create confusion matrix heat map\n",
    "f, axes = plt.subplots(figsize=(20,10))\n",
    "im = axes.imshow(confusion_train, interpolation=\"nearest\", cmap=plt.cm.Reds)\n",
    "\n",
    "axes.figure.colorbar(im, ax=axes)\n",
    "axes.set(title=\"Confusion Matrix for Training Set\", \\\n",
    "         xticks=np.arange(confusion_train.shape[1]), \\\n",
    "         yticks=np.arange(confusion_train.shape[0]), \\\n",
    "         xticklabels=range(1, 6), yticklabels=range(1, 6), \\\n",
    "         xlabel=\"Predicted\", ylabel=\"Truth\")\n",
    "\n",
    "#add clear annotations to the confusion matrix\n",
    "threshold = confusion_train.max()/1.5\n",
    "for i in range(confusion_train.shape[0]):\n",
    "    for j in range(confusion_train.shape[1]):\n",
    "        axes.text(j, i, f\"{confusion_train[i, j]:.3f}%\",\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if confusion_train[i, j] > threshold else \"black\")\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fairly compare our boosting results with our Random Forest outcome, we perform cross-validation on three folds of the training data set as well.\n",
    "\n",
    "However, since the XGBoost implementation we used is not supported by *scikit-learn*'s `.fit` method, the cross-validation must be done using `xgboost`'s own API. The output `boost_cv` is actually a *pandas* dataframe that tabulates the results of the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "boost_cv = xgb.cv(dtrain=train_set, params=parameters, nfold=3, \\\n",
    "                  num_boost_round=50, early_stopping_rounds=10, \\\n",
    "                  metrics=\"merror\", as_pandas=True, seed=42)\n",
    "\n",
    "display(boost_cv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the training set cross-validation score by getting the *merror* mean on the 50th `num_boost_round`, which is the final boosting phase. The *merror* is an accuracy error rate metric meant for multi-class labels.\n",
    "\n",
    "We can get a sense of how accurate the model is by subtracting the *merror* value from a perfect score of 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_accuracy = boost_cv.iloc[-1,2]\n",
    "cross_val_accuracy = 1-cross_val_accuracy\n",
    "\n",
    "print(f\"Training Set Accuracy: {cross_val_accuracy*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Scores ##\n",
    "Seeing that the boosting model outperformed the Random Forest approach in the three-fold cross validation, we can now apply our model on the testing set that we have put aside early on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = boost.predict(test_set)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "y_pred = y_pred+1\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average=\"micro\")\n",
    "\n",
    "print(f\"Test Set Accuracy: {accuracy*100:.3f}%\")\n",
    "print(f\"Test Set F1 Score: {f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the confusion matrix of the test set\n",
    "confusion_train = confusion_matrix(y_test, y_pred)\n",
    "confusion_train = confusion_train.astype(\"float\") / \\\n",
    "                   confusion_train.sum(axis=1)[:, np.newaxis]\n",
    "confusion_train = np.around(confusion_train, decimals=3)*100\n",
    "\n",
    "#create confusion matrix heat map\n",
    "f, axes = plt.subplots(figsize=(20,10))\n",
    "im = axes.imshow(confusion_train, interpolation=\"nearest\", cmap=plt.cm.Reds)\n",
    "\n",
    "axes.figure.colorbar(im, ax=axes)\n",
    "axes.set(title=\"Confusion Matrix for Test Set\", \\\n",
    "         xticks=np.arange(confusion_train.shape[1]), \\\n",
    "         yticks=np.arange(confusion_train.shape[0]), \\\n",
    "         xticklabels=range(1, 6), yticklabels=range(1, 6), \\\n",
    "         xlabel=\"Predicted\", ylabel=\"Truth\")\n",
    "\n",
    "#add clear annotations to the confusion matrix\n",
    "threshold = confusion_train.max()/1.5\n",
    "for i in range(confusion_train.shape[0]):\n",
    "    for j in range(confusion_train.shape[1]):\n",
    "        axes.text(j, i, f\"{confusion_train[i, j]:.3f}%\",\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if confusion_train[i, j] > threshold else \"black\")\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results above were actually based on the original `model_df` dataset that had the massive class imbalance. Let's now reassign our `X` and `y` variables to the balanced `trimmed_df` sample dataset we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trimmed_df.iloc[:, :-1]\n",
    "y = trimmed_df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_shifted = y_train-1\n",
    "y_test_shifted = y_test-1\n",
    "\n",
    "train_set = xgb.DMatrix(X_train, label=y_train_shifted)\n",
    "test_set = xgb.DMatrix(X_test, label=y_test_shifted)\n",
    "\n",
    "y_pred = boost.predict(test_set)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "y_pred = y_pred+1\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average=\"micro\")\n",
    "\n",
    "print(f\"Balanced Test Set Accuracy: {accuracy*100:.3f}%\")\n",
    "print(f\"Balanced Test Set F1 Score: {f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the confusion matrix of the balanced test set\n",
    "confusion_train = confusion_matrix(y_test, y_pred)\n",
    "confusion_train = confusion_train.astype(\"float\") / \\\n",
    "                   confusion_train.sum(axis=1)[:, np.newaxis]\n",
    "confusion_train = np.around(confusion_train, decimals=3)*100\n",
    "\n",
    "#create confusion matrix heat map\n",
    "f, axes = plt.subplots(figsize=(20,10))\n",
    "im = axes.imshow(confusion_train, interpolation=\"nearest\", cmap=plt.cm.Reds)\n",
    "\n",
    "axes.figure.colorbar(im, ax=axes)\n",
    "axes.set(title=\"Confusion Matrix for Balanced Test Set\", \\\n",
    "         xticks=np.arange(confusion_train.shape[1]), \\\n",
    "         yticks=np.arange(confusion_train.shape[0]), \\\n",
    "         xticklabels=range(1, 6), yticklabels=range(1, 6), \\\n",
    "         xlabel=\"Predicted\", ylabel=\"Truth\")\n",
    "\n",
    "#add clear annotations to the confusion matrix\n",
    "threshold = confusion_train.max()/1.5\n",
    "for i in range(confusion_train.shape[0]):\n",
    "    for j in range(confusion_train.shape[1]):\n",
    "        axes.text(j, i, f\"{confusion_train[i, j]:.3f}%\",\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if confusion_train[i, j] > threshold else \"black\")\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 53.3% on a perfectly balanced training data set, we have achieved a better result compared to the 20% accuracy of our baseline.\n",
    "\n",
    "## Word Cloud ##\n",
    "Using the true labels of the reviews, we can take the fifty most salient words in every rating and produce a word cloud. The same `stop_words` we derived from the NLTK library are excluded.\n",
    "\n",
    "We see that some of the words are quite descriptive of the rating, with \"problem\" and \"issue\" frequently appearing in one-star reviews, and \"quality\" and \"highly recommend\" in top reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(stopwords = set(stop_words), min_font_size=10, \\\n",
    "                      max_font_size=50, max_words=50, \\\n",
    "                      background_color=\"white\", colormap = \"Oranges\")\n",
    "\n",
    "one_star_text = \" \".join(df[df[\"overall\"]==1][\"reviewText\"].values).lower()\n",
    "two_star_text = \" \".join(df[df[\"overall\"]==2][\"reviewText\"].values).lower()\n",
    "three_star_text = \" \".join(df[df[\"overall\"]==3][\"reviewText\"].values).lower()\n",
    "four_star_text = \" \".join(df[df[\"overall\"]==4][\"reviewText\"].values).lower()\n",
    "five_star_text = \" \".join(df[df[\"overall\"]==5][\"reviewText\"].values).lower()\n",
    "\n",
    "text_list = [one_star_text, two_star_text, three_star_text, \\\n",
    "             four_star_text, five_star_text]\n",
    "\n",
    "for index, text in enumerate(text_list):\n",
    "    f, axes = plt.subplots(figsize=(10,7))\n",
    "    wordcloud.generate(text)\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.title(f\"Word Cloud for {index+1}-Star Ratings\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "A lot of Natural Language Processing techniques were covered in the study. Just some of the concepts explored include topic modeling â€“ where similar texts were clustered together according to topic, named entity recognition (NER) â€“ where nouns were given identifying labels like *place* or *time*, and dependency trees â€“ where parts-of-speech tags and sentence structure were discerned. Though the *Word2Vec* phase was central to our final model, the pre-processing steps were perhaps just as crucial. Prior to tokenization, each document had to be decoded from UTF and encoded to ASCII, and converted to lowercase. The texts were stripped of accents, stop words and punctuation, and multiple whitespaces were dropped. Words were simplified to their root words in order to compact the vocabulary as much as possible. Tokens that were often used together were also singularized through phrase modeling.\n",
    "\n",
    "Beyond word use and word frequency, our model actually extracts and quantifies *context*. Every token in all the reviews are understood by their neighboring words and embedded in a given number of dimensions. All the interactions of a word with all the other words it has been associated with are expressed in vectors. And all the words in a given review are averaged according to each of the dimensions to create its 100 features. So the essence of a review by its words make up the final dataframe.\n",
    "\n",
    "What we have is a multi-class model where each of the five classes correspond to a reviewâ€™s star rating. This is then a discrete approach where each class is independent of each other. In a situation where a 5-star rating is misinterpreted by the model as a 1-star review, then the model has simply misclassified â€“ it is agnostic to how far off `1` and `5` are. This is in contrast with a *continuous* approach whereas a misclassification of a 5-star review as a 1-star review would be more penalizing. Our model then is reliant on the distinction of each kind of review. It is more concerned in asking \"*What makes a 5-star review different from a 4-star review?*\" than asking \"*Is this review more approving than criticizing?*\"\n",
    "\n",
    "## Limitations and Recommendations ##\n",
    "Though we have observed satisfactory results in our model compared to the baseline, there are several limitations in the way the model handles data. These could serve as areas of improvement. First, despite a rich vocabulary, the model will not be able to handle words that it has not encountered during training. In fact, if an unknown word appears in a review, the word is dropped from the dimension-averaging step since has not  been referenced in our `word_vec_df`.\n",
    "\n",
    "Because each word is simplified by lemmatization during pre-processing, then alternate forms of a token shouldnâ€™t necessarily be a concern. However, the model cannot identify if a word is misspelled and will identify one simply as a new word. Incorporating a spellchecker would add to the computational cost and will certainly add to the modelâ€™s complexity.\n",
    "\n",
    "Finally, as is usually the case in NLP, sarcasm or text that is intended to be ironic is interpreted by what is literally in the text and not by its underlying context. Because sarcasm is usually detected by readers through the mood and sentiment of the document, it takes adding another layer of NLP just to approximate whether the review is sarcastic or not in order to properly work with such text. This supplement layer will not only utilize tagged sarcastic text as supervised labels, but must also consider the reviewâ€™s given product rating in its judgment to detect sarcasm."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
